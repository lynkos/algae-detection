{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "1b9c0d34-4c39-4175-c58f-4bd892d9e159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.1.47 ðŸš€ Python-3.11.5 torch-2.2.2 CPU (Apple M3 Max)\n",
            "Setup complete âœ… (16 CPUs, 64.0 GB RAM, 669.1/1858.2 GB disk)\n"
          ]
        }
      ],
      "source": [
        "from os import getcwd\n",
        "from os.path import dirname\n",
        "from IPython.display import clear_output\n",
        "from ultralytics import checks\n",
        "\n",
        "ROOT = dirname(getcwd())\n",
        "clear_output()\n",
        "checks()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Custom Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Create or login to a Roboflow [account](https://app.roboflow.com/login)\n",
        "2. Create a new project in the Roboflow [dashboard](https://app.roboflow.com)\n",
        "3. Choose \"Object Detection\" for Project Type\n",
        "4. [Add dataset images to project](https://docs.roboflow.com/adding-data/object-detection); upload each class in separate batches, and tag batch with class name\n",
        "5. Label images with [Roboflow Annotate](https://docs.roboflow.com/annotate) and divide the work evenly amongst teammates\n",
        "6. Go to Versions, include preprocessing and augmentations, and generate a new dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Existing Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create new directory to store dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import mkdir\n",
        "from os.path import join, exists\n",
        "\n",
        "DATASET_ROOT = join(ROOT, \"datasets\")\n",
        "if not exists(DATASET_ROOT): mkdir(DATASET_ROOT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Replace constants with your Roboflow API Key, workspace name, project name, and dataset version respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "API_KEY = \"OZYVRHRpfy8DaxBPca6s\"\n",
        "WORKSPACE = \"capstone2algae\"\n",
        "PROJECT_NAME = \"algae-detection-1opyx\"\n",
        "VERSION = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download dataset into datasets directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Dependency ultralytics==8.0.196 is required but found version=8.1.47, to fix: `pip install ultralytics==8.0.196`\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading Dataset Version Zip in algae-detection-10 to yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5879/5879 [00:00<00:00, 11946.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Extracting Dataset Version Zip to algae-detection-10 in yolov8:: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2024/2024 [00:00<00:00, 12630.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/kiran/Documents/workspace/Projects/algae-detection/src/algae-detection-10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "%cd {DATASET_ROOT}\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "PROJECT = Roboflow(api_key = API_KEY).workspace(WORKSPACE).project(PROJECT_NAME)\n",
        "PROJECT_VERSION = PROJECT.version(VERSION)\n",
        "DATASET = PROJECT_VERSION.download(\"yolov8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjFBKKqXa-u"
      },
      "source": [
        "## Custom Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import device\n",
        "from torch.cuda import is_available as is_cuda_available\n",
        "from torch.backends.mps import is_available as is_mps_available\n",
        "from ultralytics import YOLO\n",
        "\n",
        "DATASET_PATH = DATASET.location\n",
        "DATASET_CFG = join(DATASET_PATH, \"data.yaml\")\n",
        "CUSTOM_MODEL = YOLO(model = \"yolov8n\", task = \"detect\")\n",
        "DEVICE = device(\"mps\" if is_mps_available() else \"cuda\" if is_cuda_available() else \"cpu\")\n",
        "CONFIDENCE = 0.25\n",
        "EPOCHS = 25\n",
        "IMG_SIZE = 256"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[Train YOLOv8](https://docs.ultralytics.com/modes/train) on [Detect](https://docs.ultralytics.com/tasks/detect) datasets. Model is automatically validated after training is finished, unless toggled off."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpyuwrNlXc1P",
        "outputId": "0a1b7ba8-54a2-40c7-8f6b-514153656c1e"
      },
      "outputs": [],
      "source": [
        "%cd {ROOT}\n",
        "\n",
        "CUSTOM_MODEL.train(data = DATASET_CFG, plots = True, epochs = EPOCHS, imgsz = IMG_SIZE, device = DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After training, view contents of directory containing saved training results. Newly trained weights are located at `{ROOT}/runs/detect/train/weights/best.pt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "TRAIN_DIR = join(ROOT, \"runs\", \"detect\", \"train\")\n",
        "\n",
        "%ls {TRAIN_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to display images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "def output_img(path: str, width: int = 600, height: int = 600) -> None:\n",
        "    display(Image(filename = path, width = width, height = height))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Output some charts within the aforementioned directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_img(join(TRAIN_DIR, \"confusion_matrix.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_img(join(TRAIN_DIR, \"results.png\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output_img(join(TRAIN_DIR, \"val_batch0_pred.jpg\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validate Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the model is automatically validated after training is finished (unless toggled off), so this step is optional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUSTOM_MODEL.val(data = DATASET_CFG, imgsz = IMG_SIZE, conf = CONFIDENCE, device = DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eASbcWkQBq"
      },
      "source": [
        "## Inference with Custom Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Helper function to choose random test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from random import choice\n",
        "\n",
        "def rand_test_img(ds_path: str = DATASET_PATH) -> str:\n",
        "  test_dir = join(ds_path, \"test\", \"images\")\n",
        "  rand_img = choice(listdir(test_dir))\n",
        "  print(\"Running inference on\", rand_img)\n",
        "\n",
        "  return join(test_dir, rand_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Standard Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4bpUIibcV1l"
      },
      "outputs": [],
      "source": [
        "CUSTOM_MODEL.predict(rand_test_img(), save = True, imgsz = IMG_SIZE, conf = CONFIDENCE, device = DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAHI Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from cv2 import imread\n",
        "from numpy import ndarray\n",
        "from supervision import Detections, InferenceSlicer, LabelAnnotator, BoundingBoxAnnotator, plot_image\n",
        "\n",
        "def callback(image: ndarray) -> Detections:\n",
        "    return Detections.from_ultralytics(CUSTOM_MODEL(image, device = DEVICE, conf = CONFIDENCE)[0])\n",
        "\n",
        "test_img = imread(rand_test_img())\n",
        "slicer = InferenceSlicer(callback = callback)\n",
        "detections = slicer(image = test_img)\n",
        "annotated_img = BoundingBoxAnnotator().annotate(scene = test_img.copy(), detections = detections)\n",
        "\n",
        "plot_image(image = LabelAnnotator().annotate(scene = annotated_img, detections = detections))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- ðŸ’¡ ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.  \n",
        "- ðŸ’¡ ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format                                                             | `format` Argument | Model                     | Metadata | Arguments                                           |\n",
        "|--------------------------------------------------------------------|-------------------|---------------------------|----------|-----------------------------------------------------|\n",
        "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | âœ…        | -                                                   |\n",
        "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | âœ…        | `imgsz`, `optimize`                                 |\n",
        "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |\n",
        "| [OpenVINO](https://docs.openvino.ai/)                              | `openvino`        | `yolov8n_openvino_model/` | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | âœ…        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |\n",
        "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlpackage`       | âœ…        | `imgsz`, `half`, `int8`, `nms`                      |\n",
        "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | âœ…        | `imgsz`, `keras`, `int8`                            |\n",
        "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | âŒ        | `imgsz`                                             |\n",
        "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | âœ…        | `imgsz`                                             |\n",
        "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | âœ…        | `imgsz`, `half`, `int8`                             |\n",
        "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | âœ…        | `imgsz`                                             |\n",
        "| [NCNN](https://github.com/Tencent/ncnn)                            | `ncnn`            | `yolov8n_ncnn_model/`     | âœ…        | `imgsz`, `half`                                     |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "CUSTOM_MODEL.export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0tsVilOCPyq"
      },
      "source": [
        "## Deploy model on Roboflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Newly trained weights can be uploaded to Roboflow Deploy (i.e., run inference on custom model on a persistant, auto-scaling, cloud API)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EhBAJ2gCPZh",
        "outputId": "259decf5-1c4e-4011-a208-a2498acc30ca"
      },
      "outputs": [],
      "source": [
        "PROJECT_VERSION.deploy(model_type = \"yolov8\", model_path = TRAIN_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup\n",
        "Garbage collection and cleanup once done."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.cuda import empty_cache as empty_cuda_cache\n",
        "from torch.mps import empty_cache as empty_mps_cache\n",
        "from gc import collect\n",
        "\n",
        "def reset_model(model, has_model: bool = True) -> None:\n",
        "    if is_cuda_available(): empty_cuda_cache()\n",
        "    elif is_mps_available(): empty_mps_cache()\n",
        "    \n",
        "    if has_model: del model\n",
        "\n",
        "    collect()\n",
        "    globals().clear()\n",
        "\n",
        "%reset -f"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
